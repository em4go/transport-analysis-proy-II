---
title: "CLUSTERING LINEAS METRO"
author: "ALEJANDRO"
date: "2024-05-11"
output: 
  html_document:
    toc: true
    number_sections: false
    toc_depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(cluster)
library(FactoMineR)
library(factoextra)
library(NbClust)
library(clValid)
library(grid)
library(gridExtra)
library(geojsonsf)
library(arrow)
library(sf)
library(leaflet)
library(kableExtra)
library(dplyr)
```

# CLUSTERING DE BARRIOS

## Lectura y preparación de datos

Utilizaremos los mismos datos que los usado en el Análisis de Componentes Principales (PCA) de lineas de metro, que contiene información de capacidad, número de lineas y frecuencia de cada parada de metro. Además de una serie de medidas de cada parada dentro de la red de metro, como el número de vecinos, la cercanía y la importancia de cada parada en la red.


```{r carga}
#Obtenemos los datos del dataset completo de barrios
dataMetros <- read_parquet("../data/caracteristicas_metro.parquet")

#Como no hay faltantes no es necesario convertirlos en 0.

#Asignamos los nombres de los barrios como índices
rownames(dataMetros) <- dataMetros$stop_name
```

## Selección de variables a utilizar y preparación de datos

Nuestro objetivo es agrupar las paradas de metro en función de sus características. Por ello, eliminaremos las variables categóricas que no aportan información relevante para el análisis como el id de la parada y el nombre. También eliminamos las variables de frecuencia según el momento del día ya que estaban muy correlacionadas y alteraba nuestro análisis.
Además, escalaremos los datos para que tengan media 0 y desviación estándar 1 ya que las variables tienen magnitudes muy distintas.
```{r seleccion variables}
#Eliminamos las columnas que no son necesarias para el análisis
dataMetros2 <- dataMetros  %>% select(c("neighbours_in", "neighbours_out", "betweenness",
                                  "closeness","n_lines","capacidad_hora","metros_hora","pagerank","eigenvector"))
#Escalamos los datos.
dataMetros2 <- scale(dataMetros2, center = TRUE, scale = TRUE)

```

## Medida de distancia y tendencia de agrupamiento

Para la elección de la medida de distancia, decidiremos entre la distancia euclidea y la distancia de manhattan ya que queremos agrupar por paradas que tengan valores similares en las variables nombradas, no perfilies similares, por lo que descartamos medidas de similitud. Además, el elegir una medida de distancia nos ayudará a relacionarlo con el clustering de barrios ya realizado.
Para elegir entre la distancia euclidea y la de manhattan, realizaremos un mapa de calor y calcularemos el estadístico de Hopkins para cada una de las distancias.

```{r mapa de calor euclidea}
midist <- get_dist(dataMetros2, stand = FALSE, method = "euclidean")
fviz_dist(midist, show_labels = TRUE, lab_size = 0.3,
          gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
```

```{r mapa de calor manhattan}
midist2 <- get_dist(dataMetros2, stand = FALSE, method = "manhattan")
fviz_dist(midist2, show_labels = TRUE, lab_size = 0.3,
          gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
```
Observamos que los mapas de calor para ambas distancias son muy similares, por lo que no podemos decidir entre una u otra solo con este análisis. Así que nos apoyaremos el estadístico de Hopkins para decidir entre una u otra. Para ello, definimos una función que nos permite calcular el estadístico para cualquier tipo de distancia.

```{r funcion Hopkins}

calcular_hopkins_copia <- function (data, n, graph = TRUE, gradient = list(low = "red", 
  mid = "white", high = "blue"), seed = 123, metodo = "euclidean") 
{
  set.seed(seed)
  if (is.data.frame(data)) 
    data <- as.matrix(data)
  if (!(is.matrix(data))) 
    stop("data must be data.frame or matrix")
  if (n >= nrow(data)) 
    stop("n must be no larger than num of samples")
  if (!requireNamespace("reshape2", quietly = TRUE)) {
    stop("reshape2 package needed for this function to work. Please install it.")
  }
  data <- na.omit(data)
  rownames(data) <- paste0("r", 1:nrow(data))
  plot <- NULL
  if (graph) {
    plot <- fviz_dist(stats::dist(data), order = TRUE, show_labels = FALSE, 
      gradient = gradient)
  }
  p <- apply(data, 2, function(x, n) {
    runif(n, min(x), max(x))
  }, n)
  k <- round(runif(n, 1, nrow(data)))
  q <- as.matrix(data[k, ])
  distp = rep(0, nrow(data))
  distq = 0
  minp = rep(0, n)
  minq = rep(0, n)
  for (i in 1:n) {
    distp[1] <- get_dist(rbind(p[i, ], data[1, ]), method = metodo)
    minqi <- get_dist(rbind(q[i, ], data[1, ]), method = metodo)
    for (j in 2:nrow(data)) {
      distp[j] <- get_dist(rbind(p[i, ], data[j, ]), method = metodo)
      error <- q[i, ] - data[j, ]
      if (sum(abs(error)) != 0) {
        distq <- get_dist(rbind(q[i, ], data[j, ]), method = metodo)
        if (distq < minqi) 
          minqi <- distq
      }
    }
    minp[i] <- min(distp)
    minq[i] <- minqi
  }
  list(hopkins_stat = sum(minp)/(sum(minp) + sum(minq)), plot = plot)
}
```

```{r hopkins euclidea}
set.seed(100)
myN = c(75, 90, 110, 125)  # m
myhopkins = NULL
myseed = sample(1:1000, 10)
for (i in myN) {
  for (j in myseed) {
    tmp = calcular_hopkins_copia(data = dataMetros2, n = i, graph = FALSE, seed = j)
    myhopkins = c(myhopkins, tmp$hopkins_stat)
  }
}
summary(myhopkins)

```

```{r hopkins mahattan}
set.seed(100)
myN = c(75, 90, 110, 125)  # m
myhopkins = NULL
myseed = sample(1:1000, 10)
for (i in myN) {
  for (j in myseed) {
    tmp = calcular_hopkins_copia(data = dataMetros2, n = i, graph = FALSE, seed = j, metodo='manhattan')
    myhopkins = c(myhopkins, tmp$hopkins_stat)
  }
}
summary(myhopkins)

```
Observasmos que el estadístico de Hopkins para la distancia  de es ligeramente superior al de la distancia euclidea, por lo que utilizaremos la distancia de manhattan para el análisis de clustering, salvo para el método de k-medias ya que solo usa distancia euclídea.

## Modelos

En este análisis vamos a comparar por un lado, modelos jerárquicos, Ward y Complete, y por otro lado, modelos de partición, K-medias y K-medoides.  
Para la elección del número de clusters, combinaremos el análisis del coeficiente de Silhouette con la variabilidad intra-cluster.

## Modelos jerárquicos

Se han probado todo los modelos jerárquicos disponibles en R, (single,complete,average y ward), y pese a que todos mostrabn resultados aceptables, hemos seleccionado los 2 más completos. Por lo que solo mostraremos los resultados de los métodos Ward y Complete.

### Ward

```{r Ward}
p1 = fviz_nbclust(x = dataMetros2, FUNcluster = hcut, method = "silhouette", 
                  hc_method = "ward.D2", k.max = 10, verbose = FALSE, 
                  hc_metric = "manhattan") + labs(title = "Num. optimo clusters")
p2 = fviz_nbclust(x = dataMetros2, FUNcluster = hcut, method = "wss", 
                  hc_method = "ward.D2", k.max = 10, verbose = FALSE, 
                  hc_metric = "manhattan") + labs(title = "Num. optimo clusters")
grid.arrange(p1, p2, nrow = 1)
```
Los resultados del análisis de Silhouette nos indican que el número óptimos de clusters es 2, pero la variabilidad intra-cluster es muy alta. El siguiente número de clusters más óptimo es 6, y la variabilidad intra-cluster es mucho menor.
Por ello, elegimos 5 como el número de clusters para este método.

Realizamos el clustering con 6 clusters y los mostramos en un dendograma.

```{r}
ward <- hclust(midist2, method="ward.D2")

ward6 <- cutree(ward, k=6)
table(ward6)
```

```{r}
fviz_dend(ward, k = 6,
          cex = 0.5, color_labels_by_k = TRUE,
          rect = TRUE, palette=rainbow(7)) 
```

Ahora mostramos una proyección de los scores del PCA coloreados por los clusters obtenidos y añadiendo elipses, con el fin de observar si existe solapamiento o no entre los cluster, para así ver si el número de clusters elejidos es apropiado
```{r}
fviz_cluster(object = list(data=dataMetros2, cluster=ward6), stand = FALSE,
             ellipse.type = "convex", geom = "point", show.clust.cent = FALSE,
             labelsize = 8, palette=rainbow(7))  +
  labs(title = "Modelo jerarquico + Proyeccion PCA",
       subtitle = "Dist manhattan, Metodo Ward, K=7") +
  theme_bw() +
  theme(legend.position = "bottom")
```
Observamos que la mayoría de los clusters se separan en las dos primeras componentes principales, salvo los clusters 2 y 4 que muestran un ligero solapamiento. Realizaremos el gráfico para las componentes 2 y 3 para ver si se consiguen diferenciar o deberiamos considerar cambiar el número de clusters.

```{r}
fviz_cluster(object = list(data=dataMetros2, cluster=ward6), stand = FALSE,
             ellipse.type = "convex", geom = "point", show.clust.cent = FALSE,
             labelsize = 8, palette=rainbow(7), axes=2:3)  +
  labs(title = "Modelo jerarquico + Proyeccion PCA",
       subtitle = "Dist manhattan, Metodo Ward, K=7") +
  theme_bw() +
  theme(legend.position = "bottom")
```
En este gráfico, los clusters 2 y 4 se consiguen diferenciar, por lo que consideramos que el número de clusters es adecuado.


### Complete

```{r Average}
p1 = fviz_nbclust(x = dataMetros2, FUNcluster = hcut, method = "silhouette", 
                  hc_method = "complete", k.max = 10, verbose = FALSE, 
                  hc_metric = "manhattan") + labs(title = "Num. optimo clusters")
p2 = fviz_nbclust(x = dataMetros2, FUNcluster = hcut, method = "wss", 
                  hc_method = "complete", k.max = 10, verbose = FALSE, 
                  hc_metric = "manhattan") + labs(title = "Num. optimo clusters")
grid.arrange(p1, p2, nrow = 1)
```

Los resultados del análisis de Silhouette nos indican que el número óptimos de clusters es 2, y en el resto de casos, el coeficiente de Silhouette no varía muchos, por ello seleccionaremos 7 como el número de clusters para este método ya que la varianza intra-clusters es menor y para ser donde se forma el codo

Realizamos el clustering con 7 clusters y los mostramos en un dendograma.


```{r}
complete <- hclust(midist2, method="complete")

complete7 <- cutree(complete, k=7)
table(complete7)
```


```{r}
fviz_dend(complete, k = 7,
          cex = 0.5, color_labels_by_k = TRUE,
          rect = TRUE, palette=rainbow(7)) 
```
Ahora mostramos una proyección de los scores del PCA coloreados por los clusters obtenidos y añadiendo elipses, con el fin de observar si existe solapamiento o no entre los cluster, para así ver si el número de clusters elejidos es apropiado.

```{r}
fviz_cluster(object = list(data=dataMetros2, cluster=complete7), stand = FALSE,
             ellipse.type = "convex", geom = "point", show.clust.cent = FALSE,
             labelsize = 8, palette=rainbow(10))  +
  labs(title = "Modelo jerarquico + Proyeccion PCA",
       subtitle = "Dist manhattan, Metodo de la media, K=5") +
  theme_bw() +
  theme(legend.position = "bottom")
```
En el gráfico se observa a la perfección como los clusters se separan en las dos primeras componentes principales, por lo que consideramos que el número de clusters es adecuado. No es necesario realizar el gráfico para las componentes 2 y 3. 
Se observa que a diferencia del método Ward, encontramos un cluster con una sola observación.



## Modelos de partición

Ahora pasamos a los modelos de partición, donde se estudiaran tanto K-medias como PAM.

### K-medias
Como se ha nombrado anteriormente, el método de k-medias solo utiliza la distancia euclidea, por lo que no podemos utilizar la distancia de manhattan pese a que ha sido la seleccionada para nuestro análisis. Por lo que utilizaremos la distancia euclidea para este método.

```{r k-medias}
p1 = fviz_nbclust(x = dataMetros2, FUNcluster = kmeans, method = "silhouette", 
             k.max = 10, verbose = FALSE, diss=midist) +
  labs(title = "Numero optimo de clusters")
p2 = fviz_nbclust(x = dataMetros2, FUNcluster = kmeans, method = "wss", 
             k.max = 10, verbose = FALSE, diss=midist) +
  labs(title = "Numero optimo de clusters")
grid.arrange(p1, p2, nrow = 1)
```
Los resultados del análisis de Silhouette nos indican que el número óptimos de clusters es 2, pero la variabilidad intra-cluster es muy alta. El siguiente número de clusters más óptimo es 3, donde la variabilidad sigue siendo alta. El siguiente número de clusters más óptimo es 5, y la variabilidad intra-cluster es mucho menor. Por ello, realizaremos K-medias con 5 clusters.



```{r}
set.seed(100)
kmedias <- kmeans(dataMetros2, centers = 5, nstart = 20)
table(kmedias$cluster)
```
Pasamos a observar si existe solapamiento entre los clusters, mediante una proyección del PCA, para las componentes 1 y 2 y las componentes 2 y 3.
```{r}
p1 = fviz_cluster(object = list(data=dataMetros2, cluster=kmedias$cluster), stand = FALSE,
             ellipse.type = "convex", geom = "point", show.clust.cent = FALSE,
             labelsize = 8)  +
  labs(title = "K-MEDIAS + Proyeccion PCA",
       subtitle = "Dist euclidean, K=5") +
  theme_bw() +
  theme(legend.position = "bottom")
p2 = fviz_cluster(object = list(data=dataMetros2, cluster=kmedias$cluster), stand = FALSE,
             ellipse.type = "convex", geom = "point", show.clust.cent = FALSE,
             labelsize = 8, axes = 2:3)  +
  labs(title = "K-MEDIAS + Proyeccion PCA",
       subtitle = "Dist euclidean, K=5") +
  theme_bw() +
  theme(legend.position = "bottom")
grid.arrange(p1, p2, nrow = 1)
```
En las dos primeras componentes observamos una solapamiento entre el cluster 2 con los clusters 1, 4 y 5. Sin embargo, en la proyección de las componentes 2 y 3, observamos que el clusters 2 se consigue diferenciar del resto. Por lo que consideramos que el número de clusters es adecuado.

### PAM

```{r pam}
p1 = fviz_nbclust(x = dataMetros2, FUNcluster = pam, method = "silhouette", 
             k.max = 10, verbose = FALSE, diss=midist2) +
  labs(title = "Numero optimo de clusters")
p2 = fviz_nbclust(x = dataMetros2, FUNcluster = pam, method = "wss", 
             k.max = 10, verbose = FALSE, diss=midist2) +
  labs(title = "Numero optimo de clusters")
grid.arrange(p1, p2, nrow = 1)
```
El número de clusters más optimo según el coeficiente de Silhouette, serían 2 o 3, sin embargo, la variabilidad intra-cluster es demasiado elevada. El siguiente más óptimos es 6, donde la variabilidad intra-cluster ha bajado considerablemente pese a que sigue siendo muy alta. Pero como no va a descender mucho más, al aumentar el número de clusters, seleccionamos 6 como el número de clusters para este método.

```{r}
pam6 <- pam(dataMetros2, k = 6, metric="manhattan")
table(pam6$clustering) 
```

```{r}
p1 = fviz_cluster(object = list(data=dataMetros2, cluster=pam6$clustering), stand = FALSE,
             ellipse.type = "convex", geom = "point", show.clust.cent = FALSE,
             labelsize = 8)  +
  labs(title = "K-MEDOIDES + Proyeccion PCA",
       subtitle = "Dist manhattan, K=6") +
  theme_bw() +
  theme(legend.position = "bottom")
p2 = fviz_cluster(object = list(data=dataMetros2, cluster=pam6$clustering), stand = FALSE,
             ellipse.type = "convex", geom = "point", show.clust.cent = FALSE,
             labelsize = 8, axes=2:3)  +
  labs(title = "K-MEDOIDES + Proyeccion PCA",
       subtitle = "Dist manhattan, K=6") +
  theme_bw() +
  theme(legend.position = "bottom")
grid.arrange(p1, p2, nrow = 1)
```
En los gráficos observamos solapamiento entre los clusters centrales en las dos primeras componentes, que no se consiguen diferenciar en la proyección de las componentes 2 y 3.
Además, observamos que la distancia dentro de los clusters es  mucho más elevada que en el resto de métodos, por lo que descartaremos el método de PAM para nuestro análisis.


## Selección y validación del método de clustering

A la vista de los resultados anteriores, no sabríamos por que método decantarnos, ya que todos han proporcionado resultados similares.

Para tomar una decisión, analizaremos en primer lugar el coeficiente de Silhouette por cluster y por observación (no solamente el global como hasta ahora):

```{r}
par(mfrow = c(1,3))
plot(silhouette(ward6, midist2), col=rainbow(6), 
border=NA, main = "WARD")
plot(silhouette(complete7, midist2), col=rainbow(7), 
border=NA, main = "average")
plot(silhouette(kmedias$cluster, midist), col=rainbow(5), border=NA, main = "K-MEDIAS")
```
A la vista de los resultados, descartamos K-medias ya que presenta un coeficiente de Silhouette más bajo que los modelos de partición.
De los modelos de partición, seleccionamos el método de Ward ya que presenta un menor número de observaciones mal clasificadas que el método complete. Además, el método de Ward presenta un coeficiente de Silhouette global ligeramente mayor.


## Interpretación de los resultados del clustering

Para facilitar la interpretación del clustering, vamos a volver a mostrar los graficos de scores y loading para las componentes 1 y 2 y las componentes 2 y 3, ya que pese a que en el PCA unicamente se seleccionaros 2 componentetes, la tercera nos ayuda a terminar de diferenciar los clusters.
Además, los scores se colorearan según el cluster al que pertenecen.

```{r}
miPCA = PCA(dataMetros2,scale.unit = FALSE, ncp=3, graph = FALSE)
```

```{r}
p1 = fviz_pca_ind(miPCA, geom = "point", habillage = factor(ward6), addEllipses = FALSE, 
             palette = rainbow(6))
p2 = fviz_pca_var(miPCA)
grid.arrange(p1, p2, nrow = 1)
```
En este primer gráfico podemos observar que en ámbito general los clusters 1,5 y 6 son los que presentan mayores valores en la mayoria de variables, mientras que el cluster 3 es el que presenta los valores más bajos. Los clusters 2 y 4 presentan valores intermedios.

```{r}
p1 = fviz_pca_ind(miPCA, geom = "point", habillage = factor(ward6), addEllipses = FALSE, 
             palette = rainbow(6), axes=2:3)
p2 = fviz_pca_var(miPCA, axes=2:3)
grid.arrange(p1, p2, nrow = 1)
```
Este gráfico nos ayuda a observar cuales son los clusters con mas valor en la variable eigenvector ya que no se representa bien en las dos componentes, y observamos que los clusters 1, 5 son los que presentan mayores valores en esta variable. A su vez, observamos que los clusters 6 y 1 son los que más valor tienen en la variable pagerank.

Para complementar o ayudar a la interpretación anterior de los clusters mediante PCA, vamos a realizar un gráfico descriptivo del perfil de cada cluster para observar las diferencias entre ellos. Para ello, calcularemos la media de cada variable para cada cluster. 
```{r}
mediasCluster = aggregate(dataMetros2, by = list("cluster" = ward6), mean)[,-1]
rownames(mediasCluster) = paste0("c",1:6)
kable(t(round(mediasCluster,2)))
```

Para tener una vista gráfica, representamos el perfil medio de cada cluster en un gráfico de líneas.
```{r}
matplot(t(mediasCluster), type = "l", col = rainbow(6), ylab = "", xlab = "", lwd = 2,
        lty = 1, main = "Perfil medio de los clusters", xaxt = "n")
axis(side = 1, at = 1:ncol(dataMetros2), labels = colnames(dataMetros2), las = 2)
legend("topleft", as.character(1:6), col =rainbow(6), lwd = 2, ncol = 3, bty = "n")
```

En este gráfico se refuerzan las conclusiones observadas en los gráficos del PCA y nos permite observar otras.

El cluster 1 presenta valores altos en las variables neighbours_in, neighbours_out, pagerank y eigenvector.Mientras que presenta valores intermedios en las variables betweenness, closeness, n_lines, capacidad_hora y metros_hora.
Esto quiere decir que las paradas pertenecientes a este cluster tienen una gran importancia en la red, ya que tienen un gran nñumero de vecinos, y sus vecinos también son importantes en la red. Además, son puntos críticos en la conectividad de la red. Sin embargo, no son nodos muy centrales y con muchas lineas ni capacidad.

El cluster 2, que es el cluster más numeroso presenta valores intermedios en todas las variables, por lo que no se puede destacar ninguna característica en concreto. Estas paradas son las más comunes en la red.

El cluster 3 presenta valores bajos en todas las variables, especialmente en las variables neighbours_in, neighbours_out y  pagerank. Y valores más intermedios en las variables eigenvector, n_lines, capacidad_hora y metros_hora. Por lo que este cluster agrupa paradas de metro periféricas, que se conectan con muy pocas paradas, con poca importancia en la red y sin mucha alfuencia.

El cluster 4 presenta valores intermedios en las variables neighbours_in, neighbours_out, pagerank y eigenvector. Teniendo valores ligeramente más altos en el resto de variables.
Esto nos indica que las observaciones de este cluster son nodos que no tienen mucha importancia en la red y tampoco se conectan con muchas paradas, pero que tienen una buena afluencia y centralidad.

El cluster 5 presenta valores  altos en todas las variables, excepto en la variable pagerank, donde presenta un valor menor. Por lo que estas paradas se caracterizan por estar conectadas con muchas paradas, que a su vez son paradas muy conectadas, además, son nodos centricos en la red, y que tienen una gran afluencia y número de lineas. Sin embargo, no son nodos muy críticos en la conectividad de la red.

El cluster 6 presenta valores muy altos en todas las variables, salvo en la variable closeness donde es más bajo y en la variable eigenvector donde tiene un valor intermedio. 
Estas paradas, son nodos conectados a muchos nodos en la red, pero que son nodos no muy conectados, además, presentan mucha centralidad y son muy importantes en la conectividad de la red, Ademas, son paradas con mucha afluencia y muchas lineas.


## MAPA

Para finalizar, 

```{r}
#Añadir los clusters al dataset original
dataMetros3<-dataMetros
dataMetros3$cluster <- ward6
```

```{r}
mapa_valencia <- leaflet() %>%
  setView(lng = -0.375, lat = 39.4667, zoom = 12) %>%
  addTiles()


palette <- colorFactor(palette =c("red","yellow","green","lightblue","blue3","purple1"), domain = dataMetros3$cluster)

mapa_valencia <- mapa_valencia %>%
  addCircleMarkers(data =dataMetros3 , lng = ~stop_lon, lat = ~stop_lat, popup=rownames(dataMetros), radius=5,color=~palette(cluster), fillOpacity =1)



mapa_valencia
```















